import express from 'express'
import { aiService } from '../services/aiService.js'
import path from 'path'
import fs from 'fs'
import { fileURLToPath } from 'url'
import OpenAI from 'openai'
import multer from 'multer'

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)
const projectRoot = path.resolve(__dirname, '../../../')

// Initialize OpenAI client
let openai: OpenAI | undefined
try {
  if (process.env.OPENAI_API_KEY) {
    openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })
    console.log('âœ… OpenAI initialized in aiRoutes')
  }
} catch (error) {
  console.error(`âŒ Failed to initialize OpenAI in aiRoutes: ${error instanceof Error ? error.message : 'Unknown error'}`)
}

// Configure multer for audio uploads
const upload = multer({ 
  dest: '/tmp',
  limits: {
    fileSize: 10 * 1024 * 1024, // 10MB limit
  }
})

const router = express.Router()

/**
 * Enhanced contextual AI response endpoint
 */
router.post('/contextual-response', async (req: any, res: any) => {
  try {
    const { userMessage, currentStep, allSteps, videoTime, moduleId } = req.body

    if (!userMessage) {
      return res.status(400).json({ 
        success: false, 
        error: 'User message is required' 
      })
    }

    console.log(`ğŸ¤– Contextual AI request for module ${moduleId}`)
    console.log(`ğŸ“ User message: "${userMessage}"`)
    console.log(`ğŸ¬ Current step: ${currentStep?.title || 'None'}`)
    console.log(`â° Video time: ${videoTime}s`)

    // Generate contextual response using the enhanced AI service
    const response = await aiService.generateContextualResponse(
      userMessage,
      currentStep,
      allSteps,
      videoTime
    )

    console.log(`âœ… AI response generated: ${response.substring(0, 100)}...`)

    res.json({ 
      success: true, 
      response: response 
    })
  } catch (error) {
    console.error('âŒ Contextual AI response error:', error)
    res.status(500).json({ 
      success: false, 
      error: 'Failed to generate AI response' 
    })
  }
})

// Process video with enhanced AI analysis
router.post('/process-video/:moduleId', async (req, res) => {
  try {
    const { moduleId } = req.params
    console.log(`ğŸ¤– AI processing request for module: ${moduleId}`)
    
    // Get video file path
    const videoPath = path.join(projectRoot, 'backend', 'uploads', `${moduleId}.mp4`)
    
    // Check if video file exists
    if (!fs.existsSync(videoPath)) {
      console.error(`âŒ Video file not found: ${videoPath}`)
      return res.status(404).json({ 
        success: false, 
        error: 'Video file not found',
        moduleId 
      })
    }
    
    console.log(`ğŸ“¹ Processing video: ${videoPath}`)
    
    // Process video with enhanced AI analysis
    const videoData = await aiService.processVideo(`http://localhost:8000/uploads/${moduleId}.mp4`)
    
    console.log(`âœ… AI processing completed for module: ${moduleId}`)
    console.log(`ğŸ“Š Generated ${videoData.steps?.length || 0} steps`)
    
    // Save enhanced steps to file
    const stepsDir = path.join(projectRoot, 'backend', 'src', 'data', 'steps')
    const stepsPath = path.join(stepsDir, `${moduleId}.json`)
    
    console.log(`ğŸ’¾ Saving enhanced steps to: ${stepsPath}`)
    await fs.promises.mkdir(stepsDir, { recursive: true })
    await fs.promises.writeFile(stepsPath, JSON.stringify(videoData.steps, null, 2))
    
    // Return enhanced response with all analysis data
    res.json({
      success: true,
      moduleId,
      title: videoData.title,
      description: videoData.description,
      steps: videoData.steps,
      totalDuration: videoData.totalDuration,
      transcript: videoData.transcript,
      message: 'Enhanced video analysis completed successfully'
    })
  } catch (error) {
    console.error('âŒ AI processing error:', error)
    res.status(500).json({ 
      success: false, 
      error: 'AI processing failed',
      details: error instanceof Error ? error.message : 'Unknown error'
    })
  }
})

// Test enhanced AI processing
router.get('/test/:moduleId', async (req, res) => {
  try {
    const { moduleId } = req.params
    console.log(`ğŸ§ª Testing enhanced AI processing for module: ${moduleId}`)
    
    const videoPath = path.join(projectRoot, 'backend', 'uploads', `${moduleId}.mp4`)
    
    if (!fs.existsSync(videoPath)) {
      return res.status(404).json({ 
        success: false, 
        error: 'Video file not found for testing',
        moduleId 
      })
    }
    
    const videoData = await aiService.processVideo(`http://localhost:8000/uploads/${moduleId}.mp4`)
    
    res.json({
      success: true,
      moduleId,
      testResults: {
        stepsGenerated: videoData.steps?.length || 0,
        transcriptLength: videoData.transcript?.length || 0,
        totalDuration: videoData.totalDuration,
        title: videoData.title,
        description: videoData.description
      }
    })
  } catch (error) {
    console.error('âŒ Test error:', error)
    res.status(500).json({ 
      success: false, 
      error: 'Test failed',
      details: error instanceof Error ? error.message : 'Unknown error'
    })
  }
})

// Enhanced chat with context awareness
router.post('/chat', async (req, res) => {
  try {
    const { message, context } = req.body
    
    console.log(`ğŸ’¬ Enhanced chat request: ${message}`)
    console.log(`ğŸ“‹ Context:`, context)
    
    // Use enhanced AI service for chat
    const response = await aiService.chat(message, context)
    
    res.json({
      success: true,
      message: response,
      timestamp: new Date().toISOString()
    })
  } catch (error) {
    console.error('âŒ Chat error:', error)
    res.status(500).json({ 
      success: false, 
      error: 'Chat failed',
      details: error instanceof Error ? error.message : 'Unknown error'
    })
  }
})

// Debug transcription route
router.get('/debug-transcribe/:moduleId', async (req, res) => {
  try {
    const { moduleId } = req.params
    console.log('âš™ï¸ Starting transcription for', moduleId)
    
    // Test the transcription directly
    const audioPath = path.join(projectRoot, 'backend', 'processed', `${moduleId}_speech.wav`)
    
    if (!fs.existsSync(audioPath)) {
      return res.status(404).json({ error: 'Audio file not found', path: audioPath })
    }
    
    console.log('ğŸ“ Found audio file:', audioPath)
    const audioFile = await fs.promises.readFile(audioPath)
    console.log('ğŸ“ Audio file size:', audioFile.length, 'bytes')
    
    // Test OpenAI transcription
    if (!openai) {
      // Initialize OpenAI if not already done
      try {
        if (process.env.OPENAI_API_KEY) {
          openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })
          console.log('âœ… OpenAI initialized in debug route')
        } else {
          return res.status(500).json({ error: 'OpenAI API key not found' })
        }
      } catch (error) {
        return res.status(500).json({ error: 'Failed to initialize OpenAI' })
      }
    }
    
    console.log('ğŸ¤ Testing Whisper transcription...')
    console.log('ğŸ“ File size:', audioFile.length, 'bytes')
    
    // Use fs.createReadStream for OpenAI (this is the recommended approach)
    const transcription = await openai.audio.transcriptions.create({
      file: fs.createReadStream(audioPath),
      model: 'whisper-1',
      response_format: 'text'
    })
    
    console.log('âœ… Transcription successful:', transcription.length, 'characters')
    
    res.json({
      success: true,
      moduleId,
      transcription: transcription,
      length: transcription.length
    })
  } catch (error) {
    console.error('âŒ Transcription debug error:', error)
    res.status(500).json({ 
      success: false, 
      error: 'Transcription failed',
      details: error instanceof Error ? error.message : 'Unknown error'
    })
  }
})

// Real-time audio transcription endpoint
router.post('/transcribe', upload.single('audio'), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ 
        success: false, 
        error: 'No audio file provided' 
      })
    }

    console.log('ğŸ¤ Audio transcription request received')
    console.log(`ğŸ“ File: ${req.file.originalname}`)
    console.log(`ğŸ“Š Size: ${req.file.size} bytes`)

    // Read the audio file
    const audioBytes = fs.readFileSync(req.file.path)
    console.log(`ğŸ“ Audio file read: ${audioBytes.length} bytes`)

    // Initialize OpenAI if not already done
    if (!openai) {
      try {
        if (process.env.OPENAI_API_KEY) {
          openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })
          console.log('âœ… OpenAI initialized for transcription')
        } else {
          return res.status(500).json({ error: 'OpenAI API key not found' })
        }
      } catch (error) {
        return res.status(500).json({ error: 'Failed to initialize OpenAI' })
      }
    }

    console.log('ğŸ¤ Starting Whisper transcription...')
    
    // Use OpenAI Whisper for transcription
    const transcription = await openai.audio.transcriptions.create({
      file: fs.createReadStream(req.file.path),
      model: 'whisper-1',
      response_format: 'text'
    })

    // Clean up the temporary file
    fs.unlinkSync(req.file.path)

    console.log('âœ… Transcription successful:', transcription.length, 'characters')
    console.log('ğŸ“ Transcript:', transcription.substring(0, 100) + '...')

    res.json({
      success: true,
      transcript: transcription,
      length: transcription.length
    })
  } catch (error) {
    console.error('âŒ Transcription error:', error)
    
    // Clean up file on error
    if (req.file && fs.existsSync(req.file.path)) {
      fs.unlinkSync(req.file.path)
    }
    
    res.status(500).json({ 
      success: false, 
      error: 'Transcription failed',
      details: error instanceof Error ? error.message : 'Unknown error'
    })
  }
})

export { router as aiRoutes } 